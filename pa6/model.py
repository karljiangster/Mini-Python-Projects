# CS121 Linear regression
#
# Karl Jiang, karljiang

import numpy as np
from asserts import assert_Xy, assert_Xbeta


#############################
#                           #
#  Our code: DO NOT MODIFY  #
#                           #
#############################


def prepend_ones_column(A):
    '''
    Add a ones column to the left side of an array

    Inputs: 
        A: a numpy array

    Output: a numpy array
    '''
    ones_col = np.ones((A.shape[0], 1))
    return np.hstack([ones_col, A])


def linear_regression(X, y):
    '''
    Compute linear regression. Finds model, beta, that minimizes
    X*beta - Y in a least squared sense.

    Accepts inputs with type array
    Returns beta, which is used only by apply_beta

    Examples
    --------
    >>> X = np.array([[5, 2], [3, 2], [6, 2.1], [7, 3]]) # predictors
    >>> y = np.array([5, 2, 6, 6]) # dependent
    >>> beta = linear_regression(X, y)  # compute the coefficients
    >>> beta
    array([ 1.20104895,  1.41083916, -1.6958042 ])
    >>> apply_beta(beta, X) # apply the function defined by beta
    array([ 4.86363636,  2.04195804,  6.1048951 ,  5.98951049])
    '''
    assert_Xy(X, y, fname='linear_regression')

    X_with_ones = prepend_ones_column(X)

    # Do actual computation
    beta = np.linalg.lstsq(X_with_ones, y)[0]

    return beta


def apply_beta(beta, X):
    '''
    Apply beta, the function generated by linear_regression, to the
    specified values

    Inputs:
        model: beta as returned by linear_regression
        Xs: 2D array of floats

    Returns:
        result of applying beta to the data, as an array.

        Given:
            beta = array([B0, B1, B2,...BK])
            Xs = array([[x11, x12, ..., x0K],
                        [x21, x22, ..., x1K],
                        ...
                        [xN1, xN2, ..., xNK]])

            result will be:
            array([B0+B1*x11+B2*x12+...+BK*x1K,
                   B0+B1*x21+B2*x22+...+BK*x2K,
                   ...
                   B0+B1*xN1+B2*xN2+...+BK*xNK])
    '''
    assert_Xbeta(X, beta, fname='apply_beta')

    # Add a column of ones
    X_incl_ones = prepend_ones_column(X)

    # Calculate X*beta
    yhat = np.dot(X_incl_ones, beta)
    return yhat


def read_file(filename):
    '''
    Read data from the specified file.  Split the lines and convert
    float strings into floats.  Assumes the first row contains labels
    for the columns.

    Inputs:
      filename: name of the file to be read

    Returns:
      (list of strings, 2D array)
    '''
    with open(filename) as f:
        labels = f.readline().strip().split(',')
        data = np.loadtxt(f, delimiter=',', dtype=np.float64)
        return labels, data


###############
#             #
#  Your code  #
#             #
###############


class Model:
    ''' 
    Model class that holds 
    1) predictor columns
    2) dependent column
    3) beta 

    Also holds some constant strings to be used in later tasks. 
    ''' 

    VARIABLES = "Variables"
    R2 = "R^2"
    INDEXES = "Indexes"
 
    def __init__(self, variables, data, dep_var_index, pre_var_indexes):
        '''
        Initializes the attributes mentioned in the Class comments. 

        Inputs: 
            variables: all variables in the dataset. Namlely, 
            the predictor variables and the dep_var (np array of the names)
            data: the datapoints for each variable (np array/matrix 
                of integers)
            dep_var_index: the dependent variable index (in list of pre_vars)
            pre_var_indexes: list of indexes that will be used as 
            predictor variables 
        ''' 
        self.pre_var = [variables[i] for i in pre_var_indexes] #predicctor var names
        self.pre_col = data[:,pre_var_indexes] #data on predictor variables
        self.dep_col = data[:,dep_var_index] #DV data 
    
        self.beta = linear_regression(self.pre_col, self.dep_col)


def compute_r_squared(beta, pre_data, dep_col):
    ''' 
    The R^2 value obtained by computing R2 using
    β from the model on the data that was used to train the model. 
    ''' 

    Pb = apply_beta(beta, pre_data)

    var_y_yhat = 0 
    var_y = 0

    dep_var_mean = np.mean(dep_col)
    for i, actual_value in enumerate(dep_col): 
        var_y_yhat += (actual_value - Pb[i] )**2 
        var_y += (actual_value - dep_var_mean)**2

    return float(1 - var_y_yhat/var_y)

def print_model(variables, r_squared): 
    '''
    Returns a nicely outputted string Given: 

    Inputs: 
        variables: np.array or list of predictive variable names 
        r_squared: float 
    ''' 
    return ", ".join(variables) + " | R^2: " + "{0:.2f}".format(r_squared) 


def r_squared_table(variables, data, dep_var_index): 
    '''
    Returns a table of all the R-squared values for each predictor variable
    For task1a.
    ''' 
    table_data = [] 

    for index, col_name in enumerate(variables):
        if index != dep_var_index: 
            model = Model( variables, data, dep_var_index, [index] )
            r2 = compute_r_squared(model.beta, model.pre_col, model.dep_col)
            table_data.append(print_model( model.pre_var, r2) )
    
    return table_data


def get_highest_pair(variables, data, iv_indexes, dep_var_index): 
    '''
    Returns the highest r squared value among all possible 
    pairs of predictor variables. The pair is also returned. For task 2
    ''' 
    max_pair_index = [ iv_indexes[0], iv_indexes[1] ] #first two 
    model = Model(variables, data, dep_var_index, max_pair_index)
    max_r_squared = compute_r_squared(model.beta, model.pre_col, model.dep_col)

    for i in range( len (iv_indexes) ):
        for j in range(i + 1, len(iv_indexes) ): 
            model = Model(variables, data, dep_var_index, [i, j] )
            r_squared = compute_r_squared(model.beta, model.pre_col, \
                model.dep_col)
            if max_r_squared < r_squared: 
                max_r_squared = r_squared
                max_pair_index = [i,j]

    return print_model( [variables[i] for i in max_pair_index], max_r_squared)

def forward_selection_r_squared(variables, data, var_indexes, \
    dep_col_index, incl_indexes = [], incl_dict = {}, k = 1, r = 0): 
    '''
    Outputs a table that lists K, the predictor variables in the best 
    K-variable model chosen by the forward selection algorithm, and R2 
    for this model for 1≤K≤N.

    Inputs: 
        var_indexes: the predictor variables that have not yet been used. 
        Starts off as [0,1,...N]
        dep_col: numpy array of the dependent variable 
        k: the number of predictors variables s.t. 1≤K≤N (N is the size 
        of self.pre_var) 
        incl_indexes: the predictor variables that are already in the regression
        incl_dict: dictionary with keys K and values of the variable names and their 
        coresponding r^2 values. (value in form of string)
        r: current r^2 value, which is the max r^2 from the previous 
        call.  
    ''' 
    if not var_indexes: 
        return incl_dict
    
    added_r = forward_select(variables, data, var_indexes, incl_indexes, \
        dep_col_index)

    col_names = [variables[i] for i in incl_indexes]
    incl_dict[k] = {Model.VARIABLES : col_names, Model.R2 : added_r, \
    Model.INDEXES : incl_indexes} 

    return forward_selection_r_squared(variables, data, \
    var_indexes, dep_col_index, incl_indexes, incl_dict, k + 1, added_r)

def forward_select(variables, data, var_indexes, incl_indexes, dep_col_index): 
    '''
    adds the best variable index (gives the highest increase
    in r - squared.) to incl_indexes and removes that variable index from 
    var_indexes. 

    Used in the forward_selection_r_squared function. 

    Returns: 
        r: int of the highest r^2 value 
    '''

    #set inital best_r, which will be returned 
    best_pre_var_index = var_indexes[0] 
    indexes = incl_indexes + [best_pre_var_index]
    m1 = Model(variables, data, dep_col_index, indexes)
    best_r = compute_r_squared(m1.beta, m1.pre_col, m1.dep_col)
    #loop to find the highest r value  
    for index in var_indexes:
        indexes = incl_indexes + [index]
        m1 = Model(variables, data, dep_col_index, indexes)
        r_added_var = compute_r_squared(m1.beta, m1.pre_col, m1.dep_col)
        if r_added_var > best_r:
            best_r = r_added_var
            best_pre_var_index = index

    incl_indexes += [best_pre_var_index]
    var_indexes.remove(best_pre_var_index)
    return best_r 

def output_sel(sel_dict): 
    '''
    To output the dictionary in as a readable string. For task 3a
    ''' 
    rv = "" 
    for k in range(1, len(sel_dict) + 1): #dictionary keys are random so I do this 
        rv  += "k: {} | {} \n".format( k, print_model(\
            sel_dict[k][Model.VARIABLES], sel_dict[k][Model.R2] ) )

    return rv

def select_best_k(sel_dict, threshold): 
    '''
    Stops increasing K once the improvement in R2 between the model
    with K variables and the model with K-1 variables is strictly less
    than a specified threshold. Returns the model with K-1 variables.
    ''' 
    for k in range(2, len(sel_dict) + 1): #start at k = 2 since k - 1
        if sel_dict[k][Model.R2] - sel_dict[k - 1][Model.R2] < threshold: 
            return sel_dict[k-1]
    return sel_dict[k]

def test_foward_select(sel_indexes, variables, data, t_data, dep_index): 
    '''
    Tests the forward select algo in part 3a on the testing data. 

    Inputs: (variables and data are clear)
        sel_indexes: list of indexes based on forward selection 
        test_iv: a np.array for the test data of the predictor variables 
        test_dv: a np.array for the test data of the dependent variables
    
    Returns: 
        table that lists K, the predictor variables in the best 
        K-variable model chosen by the forward selection algorithm using
        the training data, and R2 for this using the model on the testing
        data for 1≤K≤N
    ''' 

    rv = "" 
    pre_var_indexes = []
    for index in sel_indexes:

        pre_var_indexes += [index]
        model = Model(variables, data, dep_index, pre_var_indexes)
        iv_data = t_data[:,pre_var_indexes]

        r2 = compute_r_squared(model.beta, iv_data, t_data[:,dep_index])
        rv  += "k: {} | {} \n".format( len(pre_var_indexes), print_model(\
            [variables[i] for i in pre_var_indexes] , r2 ) )

    return rv

